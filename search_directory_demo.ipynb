{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Directory Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the `SearchDirectory` class to create embeddings and query a set of documents using a FAISS index.\n",
    "\n",
    "The process can be broken down into the following steps:\n",
    "1. Get text information from documents in a directory\n",
    "2. Chunk the text data\n",
    "3. Load an embedding model\n",
    "4. Embed the chunked text\n",
    "5. Create a FAISS index\n",
    "6. Use a query to search the FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TYWILSON\\OneDrive - HC-SC PHAC-ASPC\\Documents\\GitHub\\faiss-search\\.venv\\lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from faiss_search import SearchDirectory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get text information from documents in a directory\n",
    "\n",
    "This step can be skipped if you already have a CSV containing the document names/file paths and the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a path to save the chunking, embedding, and faiss index to\n",
    "os.makedirs('sample_search_docs', exist_ok=True)\n",
    "search_dir_path = \"sample_search_docs\"\n",
    "\n",
    "# create a SearchDirectory object\n",
    "search = SearchDirectory(search_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 20 files completed [00:00, 50.49 files completed/s]\n",
      "Processing batches: 1 batches completed [00:00,  2.51 batches completed/s]\n"
     ]
    }
   ],
   "source": [
    "# specify the path with the files to extract text from\n",
    "resource_path = \"tests/resources/sample_text_files\"\n",
    "\n",
    "# generate a CSV report that contains text information\n",
    "search.report_from_directory(resource_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chunk the text data from the report\n",
    "\n",
    "You can either pass no arguments and it will use the `report.csv` generated in the previous step or you can specify the file path of another CSV file containing text data along with the column names of the file path and text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows (excluding header): 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 20/20 [00:00<00:00, 9787.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking complete and saved to 'data_chunked.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate chunks from the report\n",
    "search.chunk_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate chunks from a CSV file\n",
    "search.chunk_text(\"tests/resources/search_directory_test_files/report_modified.csv\",\n",
    "                  \"path\",\n",
    "                  \"content\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Specify the embedding model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TYWILSON\\OneDrive - HC-SC PHAC-ASPC\\Documents\\GitHub\\faiss-search\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "search.load_embedding_model(\"paraphrase-MiniLM-L3-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform embeddings on the chunked text data\n",
    "\n",
    "By default, this will split the task into batches that are saved to store progress during long computations. This is demonstrated by specifying the `batch_size` to be 20 chunks. The embeddings can also be broken down further by specifying the start and end chunks. The function is also designed to not recompute any chunks that have already been saved.\n",
    "\n",
    "Once all the chunks are computed and saved then they are combined and saved to `embeddings.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch complete and saved to embeddings (0-20).npy').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 38.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch complete and saved to embeddings (20-40).npy').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 40.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch complete and saved to embeddings (40-60).npy').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 38.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch complete and saved to embeddings (60-76).npy').\n",
      "Embeddings combined and saved to embeddings.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search.embed_text(row_start=0, row_end=-1, batch_size=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create the FAISS index\n",
    "\n",
    "Multiple different types of FAISS indexes can be created with different hyperparameters. The functionality of using and creating FAISS indexes is demonstrated in more depth in `faiss_demo.ipynb`. This class uses the same methods as that demo but will always save the FAISS index after creating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.create_flat_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Query the FAISS index and find the most similar documents\n",
    "\n",
    "Specify a query and the number of similar chunks to return (as well as any hyperparameters depending on the FAISS index used) and this will return a data frame with the most similar chunks (accoring to the embedding and FAISS models used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tests\\resources\\sample_text_files\\climate_chan...</td>\n",
       "      <td>The earth's climate is naturally variable on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tests\\resources\\sample_text_files\\history_of_c...</td>\n",
       "      <td>The Huron-Wendat of the Great Lakes Region, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tests\\resources\\sample_text_files\\origin_of_na...</td>\n",
       "      <td>Origin of the name \"Canada\"\\nToday, it seems i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file_path  \\\n",
       "10  tests\\resources\\sample_text_files\\climate_chan...   \n",
       "43  tests\\resources\\sample_text_files\\history_of_c...   \n",
       "56  tests\\resources\\sample_text_files\\origin_of_na...   \n",
       "\n",
       "                                              content  \n",
       "10  The earth's climate is naturally variable on a...  \n",
       "43  The Huron-Wendat of the Great Lakes Region, li...  \n",
       "56  Origin of the name \"Canada\"\\nToday, it seems i...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the meaning of life, the universe, and everything?\"\n",
    "search.search(query, k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up created files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"sample_search_docs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
